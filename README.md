# M2DSet-A-Multi-view-and-Multimodal-Dataset-with-4D-Radar-for-Autonomous-Driving

A robust and comprehensive perception system and benchmark is essential for the advancement of autonomous driving. Current datasets that emphasize multi-view and multimodal perception typically incorporate a combination of cameras and range sensors, such as lidar or radar. However, they often lack the ability to simultaneously encompass long-range perception, robustness, and panoramic perception. Achieving these capabilities is crucial for advancing autonomous driving systems. To address the limitation, we introduce a novel multi-view and multimodal dataset (M2DSet) that enables 360° perception tasks while promoting long-range and robust perception development. M2DSet surpasses existing datasets with higher-resolution, longer-range, and more robust multimodal sensors, including a 360° lidar, eight multi-view cameras, and a front-mounted 4D radar. It comprises 430 scenes, each spanning 20 seconds with 40 keyframes, and provides fully annotated 3D bounding boxes up to 210 meters. Notably, the dataset captures three front-view images with varying perception ranges, enhancing long-range and small object detection performance. Additionally, it encompasses diverse challenging driving scenarios, including varied road types, weather conditions, and lighting conditions. To validate the dataset's effectiveness, we conduct thorough dataset analysis and establish baselines for 3D object detection using cameras, lidar, and 4D radar. We anticipate that this unique and comprehensive dataset design will drive breakthroughs in panoramic, long-range, and robust perception, facilitating the development of advanced autonomous driving systems. 

In the spirit of fostering further research and collaboration, we will made our dataset and relevant experimental models publicly.
